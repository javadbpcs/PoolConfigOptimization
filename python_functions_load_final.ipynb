{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Datafame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_df(data):\n",
    "    '''\n",
    "    function to create dataframe for clusters data\n",
    "\n",
    "    input:\n",
    "        data (json): input data\n",
    "    output:\n",
    "        clusters_pd (pandas dataframe)\n",
    "    '''\n",
    "    \n",
    "    # Creating Dataframe for clusters\n",
    "    nested_cols = ['workflowInfo']\n",
    "\n",
    "    col_names1 = list(data[\"clusters\"][next(iter(data[\"clusters\"]))].keys())\n",
    "    col_names2 = list(data[\"clusters\"][next(iter(data[\"clusters\"]))]['workflowInfo'].keys())\n",
    "    col_names = list(set(col_names1 + col_names2))\n",
    "    col_names.remove('workflowInfo')\n",
    "\n",
    "    clusters_pd = pd.DataFrame(columns=col_names)\n",
    "    for i in range(len(data[\"clusters\"].keys())):\n",
    "        temp_dict = {}\n",
    "        cluster = list(data[\"clusters\"].keys())[i]\n",
    "        for col1 in col_names1:\n",
    "            if col1 not in nested_cols:\n",
    "                temp_dict[col1] = data[\"clusters\"][cluster][col1]\n",
    "        for col2 in col_names2:\n",
    "            if data[\"clusters\"][cluster]['workflowInfo'] is not None:\n",
    "                if (col2 not in nested_cols) and (col2 not in temp_dict.keys()):\n",
    "                    temp_dict[col2] = data[\"clusters\"][cluster]['workflowInfo'][col2]\n",
    "        clusters_pd.loc[i] = pd.Series(temp_dict)\n",
    "\n",
    "    # dropping unnecessary columns\n",
    "    clusters_pd = clusters_pd.drop(columns=['authenticated', 'clusterUsageTypes', 'runType'])\n",
    "\n",
    "    return clusters_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contentionTimelines_df(data):\n",
    "    \n",
    "    '''\n",
    "    function to create dataframe for contentionTimeline\n",
    "    \n",
    "    input:\n",
    "        data (json): input data\n",
    "    output:\n",
    "        clusters_pd (pandas dataframe)\n",
    "    '''\n",
    "    \n",
    "    nested_cols = ['idleCostSegment', 'clusterSegments']\n",
    "    col_names1 = list(data[\"contentionTimelines\"][0].keys())\n",
    "    col_names2 = list(data[\"contentionTimelines\"][0]['idleCostSegment'].keys())\n",
    "    col_names3 = list(data[\"contentionTimelines\"][0]['clusterSegments'][0].keys())\n",
    "    col_names = col_names1 + col_names2 + col_names3\n",
    "\n",
    "    df_dict = {}\n",
    "    cnt=-1\n",
    "    for i in range(len(data[\"contentionTimelines\"])):\n",
    "        input_dict = data[\"contentionTimelines\"][i]\n",
    "        temp_dict = {}\n",
    "\n",
    "        for col1 in col_names1:\n",
    "            if col1 not in nested_cols:\n",
    "                temp_dict[col1]=input_dict[col1]\n",
    "\n",
    "        for col2 in col_names2:\n",
    "            if col2 not in nested_cols:\n",
    "                temp_dict[col2]=input_dict['idleCostSegment'][col2]\n",
    "\n",
    "        if len(input_dict['clusterSegments'])>0:        \n",
    "            for diction in input_dict['clusterSegments']:\n",
    "                level2_keys = diction.keys()\n",
    "                cnt += 1\n",
    "                df_dict[cnt] = {}\n",
    "\n",
    "                for key in level2_keys:\n",
    "                    df_dict[cnt][key] = diction[key]\n",
    "\n",
    "                for k in temp_dict.keys():\n",
    "                    df_dict[cnt][k] = temp_dict[k]\n",
    "        else:\n",
    "            cnt += 1\n",
    "            df_dict[cnt] = {}\n",
    "            for key in level2_keys:\n",
    "                df_dict[cnt][key] = None\n",
    "            for k in temp_dict.keys():\n",
    "                df_dict[cnt][k] = temp_dict[k]\n",
    "\n",
    "    contentionTimelines_pd = pd.DataFrame.from_dict(df_dict, columns=df_dict[1].keys(), orient='index')\n",
    "\n",
    "    return contentionTimelines_pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extracting job start and end times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_timeline_df(cluster_contention_jc_pd):\n",
    "    cluster_timelines_pd = cluster_contention_jc_pd.groupby(['clusterId']).agg({'startUnixTime':min,    # Sum duration per group\n",
    "                                                         'endUnixTime': max,  # get the count of networks\n",
    "                                                        }).reset_index()\n",
    "    return cluster_timelines_pd.sort_values('startUnixTime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Grouping jobs based on their timeline overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouper function has minor updates - Oct 31\n",
    "# this is the updated grouper function\n",
    "def grouper(cluster_timelines_pd, auto_shutdown):\n",
    "    cluster_groups = {}\n",
    "    cnt = 0\n",
    "\n",
    "    for cluster_id in cluster_timelines_pd['clusterId'].values:\n",
    "        cluster_temp = cluster_timelines_pd[cluster_timelines_pd.clusterId==cluster_id]\n",
    "\n",
    "        if cnt == 0:\n",
    "            cnt += 1\n",
    "            cluster_groups[cnt] = {}\n",
    "            cluster_groups[cnt]['startUnixTime_group'] = cluster_temp['startUnixTime'].values[0]\n",
    "            cluster_groups[cnt]['endUnixTime_group'] = cluster_temp['endUnixTime'].values[0] + 60*auto_shutdown\n",
    "            cluster_groups[cnt]['cluster_ids_group'] = [cluster_temp['clusterId'].values[0]]\n",
    "        else:        \n",
    "            same_group = False\n",
    "            group_starttime = cluster_groups[cnt]['startUnixTime_group']\n",
    "            group_endtime = cluster_groups[cnt]['endUnixTime_group']\n",
    "\n",
    "            if group_starttime <= cluster_temp['startUnixTime'].values[0] <= group_endtime:\n",
    "                same_group=True\n",
    "            if group_starttime <= cluster_temp['endUnixTime'].values[0] <= group_endtime:\n",
    "                same_group=True\n",
    "\n",
    "            if same_group:\n",
    "                # updating the same group\n",
    "                cluster_groups[cnt]['cluster_ids_group'].append(cluster_temp['clusterId'].values[0])  \n",
    "                cluster_groups[cnt]['startUnixTime_group'] = min(group_starttime, cluster_temp['startUnixTime'].values[0])\n",
    "                cluster_groups[cnt]['endUnixTime_group'] = max(group_endtime, cluster_temp['endUnixTime'].values[0]+60*auto_shutdown)\n",
    "            else:\n",
    "                # creating new group\n",
    "                cnt += 1\n",
    "                cluster_groups[cnt] = {}\n",
    "                cluster_groups[cnt]['startUnixTime_group'] = cluster_temp['startUnixTime'].values[0]\n",
    "                cluster_groups[cnt]['endUnixTime_group'] = cluster_temp['endUnixTime'].values[0] + 60*auto_shutdown\n",
    "                cluster_groups[cnt]['cluster_ids_group'] = [cluster_temp['clusterId'].values[0]]\n",
    "\n",
    "                    \n",
    "    return cluster_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Calculating time intervals between groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def group_intervals(cluster_groups):\n",
    "    \n",
    "    n_groups = len(cluster_groups.keys())\n",
    "    total_groups_distance = 0\n",
    "    duration_include =1*24*3600 # (one day) (better to be multiplication of days)\n",
    "    pool_24_7_on = False # if True > pool is 24/7 on\n",
    "    group_to_exclude = []\n",
    "    \n",
    "    for key in cluster_groups.keys():\n",
    "        cluster_groups[key][\"group_duration\"]=(cluster_groups[key]['endUnixTime_group'] -\n",
    "                                               cluster_groups[key]['startUnixTime_group'])\n",
    "        # calculating groups time intervals\n",
    "        if key < n_groups:\n",
    "            cluster_groups[key][\"group_distance\"]=(cluster_groups[key+1]['startUnixTime_group'] -\n",
    "                                                   cluster_groups[key]['endUnixTime_group'])\n",
    "            total_groups_distance += cluster_groups[key][\"group_distance\"]\n",
    "        else:\n",
    "            cluster_groups[key][\"group_distance\"]=None\n",
    "\n",
    "        # if the group falls out of the intended duration (duration_include)\n",
    "        if (cluster_groups[key]['startUnixTime_group'] - cluster_groups[1]['startUnixTime_group']) > duration_include:\n",
    "            group_to_exclude.append(key)\n",
    "            total_groups_distance -= cluster_groups[key-1][\"group_distance\"]\n",
    "\n",
    "    if len(group_to_exclude)>0:\n",
    "        for key in group_to_exclude:\n",
    "            del cluster_groups[key]\n",
    "        # updating n_groups\n",
    "        n_groups = len(cluster_groups.keys())\n",
    "\n",
    "    if pool_24_7_on:\n",
    "        total_groups_distance += duration_include - (cluster_groups[n_groups]['endUnixTime_group'] - cluster_groups[1]['startUnixTime_group'])\n",
    "        \n",
    "        \n",
    "    return total_groups_distance, cluster_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding group number to cluster_timelines_pd from Step 1 to be used in step 4 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_timeline_update(cluster_timelines_pd, cluster_groups):\n",
    "    '''\n",
    "    function to update the cluster_timeline_pd dataframe to include group numbers\n",
    "    '''\n",
    "    cluster_timelines_pd['group_number'] = [0]*len(cluster_timelines_pd)\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "        for cluster_id in cluster_groups[key]['cluster_ids_group']:\n",
    "            cluster_timelines_pd.loc[cluster_timelines_pd.clusterId==cluster_id, 'group_number']= key\n",
    "\n",
    "    # Note: if group_number==0 the cluster falls out of the intended duration (duration_include)\n",
    "    cluster_timelines_pd = cluster_timelines_pd[cluster_timelines_pd.group_number>0].sort_values('startUnixTime')\n",
    "    \n",
    "    \n",
    "    return cluster_timelines_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Calculating ivm-sec index of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idle_vm_find2(input_df):\n",
    "    '''\n",
    "    function to find vms as soon as they become idle at cluster level\n",
    "    outputs:\n",
    "        idle_vm_: info of vms and the time they become idle\n",
    "        release_phase: a part of dataframe after initial idle vm release\n",
    "                       which has potential of more idle vms\n",
    "    '''\n",
    "    idle_vm_ = pd.DataFrame()\n",
    "    df_max = input_df[input_df.vmCount==input_df['vmCount'].max()]\n",
    "    df_max_latest = df_max[df_max.endUnixTime==df_max['endUnixTime'].max()]\n",
    "    release_phase = input_df[input_df.startUnixTime >= df_max_latest.endUnixTime.values[0]]\n",
    "\n",
    "    if len(release_phase)>0:\n",
    "        idle_vm_['idle_vm_count'] = df_max_latest['vmCount'].values[0]-release_phase['vmCount'].max()\n",
    "        idle_vm_['availability_startUinxTime'] = df_max_latest['endUnixTime'].values[0]\n",
    "        idle_vm_['clusterId'] = df_max_latest['clusterId'].values[0]\n",
    "    else:\n",
    "        release_phase = []\n",
    "        idle_vm_['idle_vm_count'] = df_max_latest['vmCount'] #If no nodes after => all the vms will be idle\n",
    "        idle_vm_['availability_startUinxTime'] = df_max_latest['endUnixTime']\n",
    "        idle_vm_['clusterId'] = df_max_latest['clusterId']\n",
    "        \n",
    "    return idle_vm_, release_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idle_vm_find(input_df):\n",
    "    '''\n",
    "    function to find vms as soon as they become idle at cluster level\n",
    "    outputs:\n",
    "        idle_vm_: info of vms and the time they become idle\n",
    "        release_phase: a part of dataframe after initial idle vm release\n",
    "                       which has potential of more idle vms\n",
    "    '''\n",
    "    idle_vm_ = pd.DataFrame()\n",
    "    df_max = input_df[input_df.vmCount==input_df['vmCount'].max()]\n",
    "    df_max_latest = df_max[df_max.endUnixTime==df_max['endUnixTime'].max()]\n",
    "    release_phase = input_df[input_df.startUnixTime >= df_max_latest.endUnixTime.values[0]]\n",
    "\n",
    "    if len(release_phase)>0:\n",
    "        idle_vm_['idle_vm_count'] = df_max_latest['vmCount'].values[0]-release_phase['vmCount'][release_phase.startUnixTime==release_phase['startUnixTime'].min()]\n",
    "        idle_vm_['availability_startUinxTime'] = df_max_latest['endUnixTime'].values[0]\n",
    "        idle_vm_['clusterId'] = df_max_latest['clusterId'].values[0]\n",
    "    else:\n",
    "        release_phase = []\n",
    "        idle_vm_['idle_vm_count'] = df_max_latest['vmCount'] #If no nodes after => all the vms will be idle\n",
    "        idle_vm_['availability_startUinxTime'] = df_max_latest['endUnixTime']\n",
    "        idle_vm_['clusterId'] = df_max_latest['clusterId']\n",
    "        \n",
    "    return idle_vm_, release_phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idle_df2(cluster_groups, cluster_contention_jc_pd):\n",
    "    \n",
    "    appended_data = []\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "        for cluster_id in cluster_groups[key]['cluster_ids_group']:\n",
    "            #cnt = 0\n",
    "            release_phase = ['random']\n",
    "\n",
    "            cluster_df = cluster_contention_jc_pd[cluster_contention_jc_pd.clusterId==cluster_id].sort_values('startUnixTime') \n",
    "\n",
    "            while len(release_phase)>0:\n",
    "                idle_vm_, release_phase = idle_vm_find2(cluster_df)\n",
    "                cluster_df = release_phase\n",
    "                idle_vm_['group_number'] = [key]*len(idle_vm_)\n",
    "                appended_data.append(idle_vm_) \n",
    "                #cnt+=1\n",
    "                #print(cnt, cluster_id)\n",
    "\n",
    "    idle_vm_df = pd.concat(appended_data).reset_index(drop=True)  \n",
    "\n",
    "    return idle_vm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idle_df(cluster_groups, cluster_contention_jc_pd):\n",
    "    \n",
    "    appended_data = []\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "        for cluster_id in cluster_groups[key]['cluster_ids_group']:\n",
    "            #cnt = 0\n",
    "            release_phase = ['random']\n",
    "\n",
    "            cluster_df = cluster_contention_jc_pd[cluster_contention_jc_pd.clusterId==cluster_id].sort_values('startUnixTime') \n",
    "\n",
    "            while len(release_phase)>0:\n",
    "                idle_vm_, release_phase = idle_vm_find(cluster_df)\n",
    "                cluster_df = release_phase\n",
    "                idle_vm_['group_number'] = [key]*len(idle_vm_)\n",
    "                appended_data.append(idle_vm_) \n",
    "                #cnt+=1\n",
    "                #print(cnt, cluster_id)\n",
    "\n",
    "    idle_vm_df = pd.concat(appended_data).reset_index(drop=True)  \n",
    "\n",
    "    return idle_vm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating idle vm df with one idle vm per row (needed for next parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updated on Nov 12\n",
    "def idle_one_df(idle_vm_df):\n",
    "\n",
    "    idle_vm_one_df = pd.DataFrame(columns=idle_vm_df.columns)\n",
    "    idle_vm_df2 = idle_vm_df.copy()\n",
    "    cnt=0\n",
    "\n",
    "    for i in idle_vm_df.index:\n",
    "        temp_list = list(idle_vm_df.loc[i].values)\n",
    "        idle_vm_df2.loc[idle_vm_df2.index==i, 'idle_vm_count'] = 1\n",
    "        for j in range(int(temp_list[0])):\n",
    "            idle_vm_one_df.loc[cnt] = idle_vm_df2.loc[i]\n",
    "            cnt+=1\n",
    "\n",
    "    # by default the idle vm will stay on till auto_shutdown time (defined in step 2) uless resued        \n",
    "    idle_vm_one_df['availability_endUinxTime'] = idle_vm_one_df['availability_startUinxTime'] + auto_shutdown*60\n",
    "\n",
    "    # adding local vm id\n",
    "    idle_vm_one_df['local_vm_id'] = idle_vm_one_df.index + 11 # (11 is just a random starter id)\n",
    "\n",
    "    # adding vm reused status\n",
    "    idle_vm_one_df['vm_reused'] = [False] * len(idle_vm_one_df) \n",
    "\n",
    "    # sorting by availability time\n",
    "    idle_vm_one_df.sort_values('availability_startUinxTime', inplace=True)\n",
    "\n",
    "    return idle_vm_one_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - part B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vm_usage(cluster_groups, cluster_timelines_pd, idle_vm_one_df, cluster_contention_jc_pd):\n",
    "    \n",
    "    t1 = time.time()\n",
    "\n",
    "    vm_usage_classes = {}\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "\n",
    "        # group level parameters\n",
    "        warm_start_cnt_g_ = 0\n",
    "        cold_start_cnt_g_ = 0\n",
    "        cold_start_cnt_parallel_g_ = 0\n",
    "        vm_usage_classes[key] = {}\n",
    "        print(key)\n",
    "\n",
    "        idle_vm_df_g = idle_vm_one_df[(idle_vm_one_df.group_number==key) & (idle_vm_one_df.vm_reused==False)]\n",
    "        # idle_vm_df_g.sort_values('availability_startUinxTime')\n",
    "\n",
    "        cluster_timelines_df_g = cluster_timelines_pd[cluster_timelines_pd.group_number==key]\n",
    "        cluster_timelines_df_g.sort_values('startUnixTime', inplace=True)\n",
    "\n",
    "\n",
    "        for cluster_id in cluster_timelines_df_g['clusterId'].values:\n",
    "            # print(cluster_id)\n",
    "\n",
    "            # idle vms won't be used by the same cluster from where they are relased\n",
    "            idle_vm_df_c = idle_vm_df_g[(idle_vm_df_g.clusterId!=cluster_id) & (idle_vm_df_g.vm_reused==False)]\n",
    "\n",
    "\n",
    "            cluster_df = (\n",
    "                cluster_contention_jc_pd[cluster_contention_jc_pd.clusterId==cluster_id]\n",
    "                .sort_values('startUnixTime')) \n",
    "\n",
    "            # number of vms already allocated to the current cluster\n",
    "            n_vm_c_aloc = 0\n",
    "\n",
    "            for i in range(len(cluster_df)):\n",
    "\n",
    "\n",
    "               # number of vms needed at current cluster at current time\n",
    "                n_vm_c_need = int(cluster_df.iloc[i].vmCount)\n",
    "\n",
    "                # look if there is any idle vms available\n",
    "                idle_vm_df_c = idle_vm_df_c[idle_vm_df_c.vm_reused == False]\n",
    "                idles = []\n",
    "                idles = idle_vm_df_c[(idle_vm_df_c.availability_startUinxTime < cluster_df.iloc[i].startUnixTime) &\n",
    "                              (cluster_df.iloc[i].startUnixTime < idle_vm_df_c.availability_endUinxTime)]\n",
    "\n",
    "\n",
    "                parallel = False\n",
    "                # if len(idles)>0:\n",
    "                # only if we have less allocated vms than required\n",
    "                if n_vm_c_need > n_vm_c_aloc:\n",
    "                    for v in range(n_vm_c_need - n_vm_c_aloc):\n",
    "                        if v < len(idles):\n",
    "                            idle_ind = idle_vm_one_df[idle_vm_one_df.local_vm_id == idles.iloc[v]['local_vm_id']].index\n",
    "                            idle_vm_one_df.loc[idle_ind, 'availability_endUinxTime'] = cluster_df.iloc[i].startUnixTime\n",
    "                            idle_vm_df_g.loc[idle_vm_df_g.local_vm_id == idles.iloc[v]['local_vm_id'],\n",
    "                                             'vm_reused'] = True\n",
    "                            idle_vm_df_c.loc[idle_vm_df_c.local_vm_id == idles.iloc[v]['local_vm_id'],\n",
    "                                             'vm_reused'] = True\n",
    "                            idle_vm_one_df.loc[idle_ind, 'vm_reused'] = True\n",
    "                            warm_start_cnt_g_ += 1\n",
    "                        else:\n",
    "                            if parallel:\n",
    "                                cold_start_cnt_parallel_g_ += 1\n",
    "                            # when there is no idle vm or all available idle vms are already re-allocated\n",
    "                            cold_start_cnt_g_ += 1\n",
    "                            parallel = True\n",
    "\n",
    "                        n_vm_c_aloc +=1\n",
    "                    # Note: sumwhere here you should count the total active vm at pool for effect of max vm\n",
    "                    # but think about it more \n",
    "\n",
    "                    # Note: if n_vm_c_need < n_vm_c_aloc then we will get idle vm here\n",
    "                    # in the next version you can make the idle vm detection at this step for speed ups\n",
    "                    # and detecting idle vms that stay idle shortly but will be reused by the same cluster later\n",
    "                    # (this part should be added only if needed i.e when we know that if a vm is done it will be \n",
    "                    # immidialtely released to the pool eventhough it will be reused later by the same cluster Q1\n",
    "                    # on page B24)\n",
    "\n",
    "        # updating level parameters\n",
    "        vm_usage_classes[key]['cold_start_cnt'] = cold_start_cnt_g_\n",
    "        vm_usage_classes[key]['warm_start_cnt'] = warm_start_cnt_g_\n",
    "        vm_usage_classes[key]['cold_start_cnt_parallel'] = cold_start_cnt_parallel_g_\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('duration is: ', t2-t1)\n",
    "\n",
    "    ## Part e: calculating idle vm second (ivm-sec) index of each group\n",
    "    idle_vm_one_df['idle_duration'] = (idle_vm_one_df['availability_endUinxTime'] \n",
    "                                       - idle_vm_one_df['availability_startUinxTime'])\n",
    "\n",
    "    return vm_usage_classes, idle_vm_one_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vm_usage_v3(cluster_groups, cluster_timelines_pd, idle_vm_one_df, cluster_contention_jc_pd):\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    vm_usage_classes = {}\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "\n",
    "        # group level parameters\n",
    "        warm_start_cnt_g_ = 0\n",
    "        cold_start_cnt_g_ = 0\n",
    "        cold_start_cnt_parallel_g_ = 0\n",
    "        vm_usage_classes[key] = {}\n",
    "        print('**************** group number is', key)\n",
    "\n",
    "        idle_vm_df_g = idle_vm_one_df[(idle_vm_one_df.group_number==key) & (idle_vm_one_df.vm_reused==False)]\n",
    "        # idle_vm_df_g.sort_values('availability_startUinxTime')\n",
    "\n",
    "        cluster_timelines_df_g = cluster_timelines_pd[cluster_timelines_pd.group_number==key]\n",
    "        cluster_timelines_df_g.sort_values('startUnixTime', inplace=True)\n",
    "\n",
    "\n",
    "        for cluster_id in cluster_timelines_df_g['clusterId'].values:\n",
    "            # print(cluster_id)\n",
    "\n",
    "            # idle vms won't be used by the same cluster from where they are relased\n",
    "            idle_vm_df_c = idle_vm_df_g[(idle_vm_df_g.clusterId!=cluster_id) & (idle_vm_df_g.vm_reused==False)]\n",
    "\n",
    "\n",
    "            cluster_df = (\n",
    "                cluster_contention_jc_pd[cluster_contention_jc_pd.clusterId==cluster_id]\n",
    "                .sort_values('startUnixTime')) \n",
    "\n",
    "            # number of vms already allocated to the current cluster\n",
    "            n_vm_c_aloc = 0\n",
    "            \n",
    "            # max number of vms needed for the current cluster\n",
    "            n_vm_max = cluster_df['vmCount'].max()\n",
    "            print(n_vm_max)\n",
    "            \n",
    "            for i in range(len(cluster_df)):\n",
    "\n",
    "\n",
    "               # number of vms needed at current cluster at current time\n",
    "                n_vm_c_need = int(cluster_df.iloc[i].vmCount)\n",
    "                \n",
    "               # when all the vms for current cluster are already started\n",
    "                if n_vm_c_need <= n_vm_c_aloc:\n",
    "                    continue\n",
    "                    \n",
    "               # when max number of vms are already allocated   \n",
    "                if n_vm_max == n_vm_c_aloc:\n",
    "                    break\n",
    "                    \n",
    "                # look if there is any idle vms available\n",
    "                idle_vm_df_c = idle_vm_df_c[idle_vm_df_c.vm_reused == False]\n",
    "                idles = []\n",
    "                idles = idle_vm_df_c[(idle_vm_df_c.availability_startUinxTime < cluster_df.iloc[i].startUnixTime) &\n",
    "                              (cluster_df.iloc[i].startUnixTime < idle_vm_df_c.availability_endUinxTime)]\n",
    "                # print('#len idles is', len(idles))\n",
    "\n",
    "                parallel = False\n",
    "                # if len(idles)>0:\n",
    "                # only if we have less allocated vms than required\n",
    "                if n_vm_c_need > n_vm_c_aloc:\n",
    "                    for v in range(n_vm_c_need - n_vm_c_aloc):\n",
    "                        # print('difference is:', n_vm_c_need - n_vm_c_aloc)\n",
    "                        if v < len(idles):\n",
    "                            # idle_ind = idle_vm_one_df[idle_vm_one_df.local_vm_id == idles.iloc[v]['local_vm_id']].index\n",
    "                            local_vm_id = idles.iloc[v]['local_vm_id']\n",
    "                            # print('lvi is', local_vm_id)\n",
    "                            idle_vm_one_df.loc[idle_vm_one_df.local_vm_id==local_vm_id, 'availability_endUinxTime'] = cluster_df.iloc[i].startUnixTime\n",
    "                            idle_vm_df_g.loc[idle_vm_df_g.local_vm_id == local_vm_id, 'vm_reused'] = True\n",
    "                            idle_vm_df_c.loc[idle_vm_df_c.local_vm_id == local_vm_id, 'vm_reused'] = True\n",
    "                            idle_vm_one_df.loc[idle_vm_one_df.local_vm_id == local_vm_id, 'vm_reused'] = True\n",
    "                            warm_start_cnt_g_ += 1\n",
    "                        else:\n",
    "                            if parallel:\n",
    "                                cold_start_cnt_parallel_g_ += 1\n",
    "                            # when there is no idle vm or all available idle vms are already re-allocated\n",
    "                            cold_start_cnt_g_ += 1\n",
    "                            parallel = True\n",
    "\n",
    "                        n_vm_c_aloc +=1\n",
    "\n",
    "        # updating level parameters\n",
    "        vm_usage_classes[key]['cold_start_cnt'] = cold_start_cnt_g_\n",
    "        vm_usage_classes[key]['warm_start_cnt'] = warm_start_cnt_g_\n",
    "        vm_usage_classes[key]['cold_start_cnt_parallel'] = cold_start_cnt_parallel_g_\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('duration is: ', t2-t1)\n",
    "\n",
    "    ## Part e: calculating idle vm second (ivm-sec) index of each group\n",
    "    idle_vm_one_df['idle_duration'] = (idle_vm_one_df['availability_endUinxTime'] \n",
    "                                       - idle_vm_one_df['availability_startUinxTime'])\n",
    "\n",
    "    return vm_usage_classes, idle_vm_one_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vm_usage_v2(cluster_groups, cluster_timelines_pd, idle_vm_one_df, cluster_contention_jc_pd):\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "    vm_usage_classes = {}\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "\n",
    "        # group level parameters\n",
    "        warm_start_cnt_g_ = 0\n",
    "        cold_start_cnt_g_ = 0\n",
    "        cold_start_cnt_parallel_g_ = 0\n",
    "        vm_usage_classes[key] = {}\n",
    "        print('**************** group number is', key)\n",
    "\n",
    "        idle_vm_df_g = idle_vm_one_df[(idle_vm_one_df.group_number==key) & (idle_vm_one_df.vm_reused==False)]\n",
    "        # idle_vm_df_g.sort_values('availability_startUinxTime')\n",
    "\n",
    "        cluster_timelines_df_g = cluster_timelines_pd[cluster_timelines_pd.group_number==key]\n",
    "        cluster_timelines_df_g.sort_values('startUnixTime', inplace=True)\n",
    "\n",
    "\n",
    "        for cluster_id in cluster_timelines_df_g['clusterId'].values:\n",
    "            # print(cluster_id)\n",
    "\n",
    "            # idle vms won't be used by the same cluster from where they are relased\n",
    "            idle_vm_df_c = idle_vm_df_g[(idle_vm_df_g.clusterId!=cluster_id) & (idle_vm_df_g.vm_reused==False)]\n",
    "\n",
    "\n",
    "            cluster_df = (\n",
    "                cluster_contention_jc_pd[cluster_contention_jc_pd.clusterId==cluster_id]\n",
    "                .sort_values('startUnixTime')) \n",
    "\n",
    "            # number of vms already allocated to the current cluster\n",
    "            n_vm_c_aloc = 0\n",
    "\n",
    "            for i in range(len(cluster_df)):\n",
    "\n",
    "\n",
    "               # number of vms needed at current cluster at current time\n",
    "                n_vm_c_need = int(cluster_df.iloc[i].vmCount)\n",
    "\n",
    "                # look if there is any idle vms available\n",
    "                idle_vm_df_c = idle_vm_df_c[idle_vm_df_c.vm_reused == False]\n",
    "                idles = []\n",
    "                idles = idle_vm_df_c[(idle_vm_df_c.availability_startUinxTime < cluster_df.iloc[i].startUnixTime) &\n",
    "                              (cluster_df.iloc[i].startUnixTime < idle_vm_df_c.availability_endUinxTime)]\n",
    "                # print('#len idles is', len(idles))\n",
    "\n",
    "                parallel = False\n",
    "                # if len(idles)>0:\n",
    "                # only if we have less allocated vms than required\n",
    "                if n_vm_c_need > n_vm_c_aloc:\n",
    "                    for v in range(n_vm_c_need - n_vm_c_aloc):\n",
    "                        # print('difference is:', n_vm_c_need - n_vm_c_aloc)\n",
    "                        if v < len(idles):\n",
    "                            # idle_ind = idle_vm_one_df[idle_vm_one_df.local_vm_id == idles.iloc[v]['local_vm_id']].index\n",
    "                            local_vm_id = idles.iloc[v]['local_vm_id']\n",
    "                            # print('lvi is', local_vm_id)\n",
    "                            idle_vm_one_df.loc[idle_vm_one_df.local_vm_id==local_vm_id, 'availability_endUinxTime'] = cluster_df.iloc[i].startUnixTime\n",
    "                            idle_vm_df_g.loc[idle_vm_df_g.local_vm_id == local_vm_id, 'vm_reused'] = True\n",
    "                            idle_vm_df_c.loc[idle_vm_df_c.local_vm_id == local_vm_id, 'vm_reused'] = True\n",
    "                            idle_vm_one_df.loc[idle_vm_one_df.local_vm_id == local_vm_id, 'vm_reused'] = True\n",
    "                            warm_start_cnt_g_ += 1\n",
    "                        else:\n",
    "                            if parallel:\n",
    "                                cold_start_cnt_parallel_g_ += 1\n",
    "                            # when there is no idle vm or all available idle vms are already re-allocated\n",
    "                            cold_start_cnt_g_ += 1\n",
    "                            parallel = True\n",
    "\n",
    "                        n_vm_c_aloc +=1\n",
    "\n",
    "        # updating level parameters\n",
    "        vm_usage_classes[key]['cold_start_cnt'] = cold_start_cnt_g_\n",
    "        vm_usage_classes[key]['warm_start_cnt'] = warm_start_cnt_g_\n",
    "        vm_usage_classes[key]['cold_start_cnt_parallel'] = cold_start_cnt_parallel_g_\n",
    "\n",
    "    t2 = time.time()\n",
    "    print('duration is: ', t2-t1)\n",
    "\n",
    "    ## Part e: calculating idle vm second (ivm-sec) index of each group\n",
    "    idle_vm_one_df['idle_duration'] = (idle_vm_one_df['availability_endUinxTime'] \n",
    "                                       - idle_vm_one_df['availability_startUinxTime'])\n",
    "\n",
    "    return vm_usage_classes, idle_vm_one_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vm_usage_v4(cluster_groups, cluster_timelines_pd, cluster_contention_jc_pd, auto_shutdown, verbose=False):\n",
    "\n",
    "    cluster_group_pd = cluster_timelines_pd[['clusterId', 'group_number']]\n",
    "    cluster_contention_jc_pd = pd.merge(cluster_contention_jc_pd, cluster_group_pd, on='clusterId')\n",
    "    cluster_contention_jc_pd.drop(['clusterType', 'costActive', 'activeNodes', 'idleNodes', 'maxCapacity', 'cost'], axis=1,inplace=True)\n",
    "\n",
    "    vm_warm_cnt_total = 0\n",
    "    vm_cold_cnt_total = 0\n",
    "    vm_usage_classes = {}\n",
    "\n",
    "\n",
    "    for key in cluster_groups.keys():\n",
    "\n",
    "            # group level parameters\n",
    "            warm_start_cnt_g_ = 0\n",
    "            cold_start_cnt_g_ = 0\n",
    "            vm_usage_classes[key] = {}\n",
    "\n",
    "\n",
    "            cluster_df = cluster_contention_jc_pd[cluster_contention_jc_pd.group_number==key].sort_values(['clusterId','startUnixTime']) \n",
    "\n",
    "            # finding drops in the number of vms (meaning those vms becoming idle)\n",
    "            cluster_df['vm_scale'] = cluster_df['vmCount'] - cluster_df['vmCount'].shift(-1)\n",
    "            cluster_df['time_diff_fw'] = cluster_df['startUnixTime'] - cluster_df['endUnixTime'].shift(+1)\n",
    "            cluster_df['time_diff_bw'] = cluster_df['endUnixTime'] - cluster_df['startUnixTime'].shift(-1)\n",
    "\n",
    "            # at the start of each cluster your vm_scale is equal to the vmCount (new Vms count needed)\n",
    "            # time_diff_fw: for cluster starts >> vm_scale will be needed to cold start (negative)(by default)\n",
    "            # time_diff_bw: for cluster ends >> vm_scale will be idle (positive)\n",
    "            # Note: Sequence of the next two lines matters. first bw then fw\n",
    "            cluster_df['vm_scale'][cluster_df.time_diff_bw!=0] = cluster_df['vmCount']\n",
    "            cluster_df['vm_scale'][cluster_df.time_diff_fw!=0] = -cluster_df['vmCount']\n",
    "\n",
    "            vm_pos = cluster_df[cluster_df.vm_scale > 0]\n",
    "            vm_neg = cluster_df[cluster_df.vm_scale < 0]\n",
    "            idle_vm_count_g_ = vm_pos['vm_scale'].sum()\n",
    "\n",
    "            # NOTE: endUnixTime is when the change happens due to how diff was made\n",
    "            vm_pos['start_availability_time'] = vm_pos['endUnixTime']\n",
    "            vm_pos['end_availability_time'] = vm_pos['endUnixTime'] + 60*auto_shutdown\n",
    "            vm_pos['idle_duration'] = [0]*len(vm_pos)\n",
    "            vm_pos['vm_reused_cnt'] = [0]*len(vm_pos)\n",
    "            vm_pos.reset_index(inplace=True)\n",
    "\n",
    "            # adding local id\n",
    "            vm_neg.reset_index(inplace=True)\n",
    "            vm_neg['local_id'] = vm_neg.index + 11 # (11 is just a random starter id)\n",
    "\n",
    "\n",
    "            for i in range(len(vm_pos)):\n",
    "                vm_p_cnt = vm_pos.iloc[i]['vm_scale']\n",
    "                cold_starts = vm_neg.loc[(vm_neg.vm_scale<0) & (vm_pos.iloc[i].start_availability_time < vm_neg.endUnixTime) & \n",
    "                               (vm_neg.endUnixTime < vm_pos.iloc[i].end_availability_time)]\n",
    "                local_ids = cold_starts['local_id'].values\n",
    "                cold_starts_len = len(local_ids)\n",
    "\n",
    "                if cold_starts_len>0:\n",
    "                    vm_warm_cnt = 0\n",
    "                    cnt = 0\n",
    "                    while vm_warm_cnt < vm_p_cnt:\n",
    "                        if cnt < cold_starts_len:\n",
    "                            true_vm = min(abs(cold_starts.iloc[cnt]['vm_scale']), min(vm_p_cnt, vm_p_cnt-vm_warm_cnt))\n",
    "                            cold_starts.loc[cold_starts.local_id==local_ids[cnt],'vm_scale'] += true_vm\n",
    "                            idle_duration = cold_starts.iloc[cnt]['endUnixTime'] - vm_pos.iloc[i].start_availability_time\n",
    "                            vm_pos.loc[vm_pos.index==i,'idle_duration'] += idle_duration*true_vm\n",
    "                            vm_pos.loc[vm_pos.index==i,'vm_reused_cnt'] += true_vm\n",
    "                            vm_warm_cnt += true_vm\n",
    "                            vm_warm_cnt_total += true_vm\n",
    "                            vm_neg.loc[vm_neg.local_id==local_ids[cnt],'vm_scale'] += true_vm\n",
    "                            cnt += 1\n",
    "                        else:\n",
    "                            break\n",
    "\n",
    "\n",
    "            cold_start_cnt_g_ = abs(vm_neg['vm_scale'].sum())\n",
    "            warm_start_cnt_g_ = vm_pos['vm_reused_cnt'].sum()\n",
    "            vm_cold_cnt_total += cold_start_cnt_g_\n",
    "\n",
    "            # calculating idle_duration                \n",
    "            vm_pos['idle_duration'] += (vm_pos['vm_scale'] - vm_pos['vm_reused_cnt'])*auto_shutdown*60\n",
    "            idle_duration_g_ = vm_pos['idle_duration'].sum()\n",
    "            # when there is only one record for the current group >> start and end \n",
    "            # of the cluster happens at the same row\n",
    "            if len(cluster_df)==1:\n",
    "                idle_duration_g_ = cold_start_cnt_g_*auto_shutdown*60\n",
    "\n",
    "            if verbose:\n",
    "                print(f'***Group Number is {key}')\n",
    "                print('total cold starts:', cold_start_cnt_g_)\n",
    "                print('total warm starts:', warm_start_cnt_g_)\n",
    "                print('total idle duration is:', idle_duration_g_)\n",
    "\n",
    "            # updating level parameters\n",
    "            vm_usage_classes[key]['cold_start_cnt'] = cold_start_cnt_g_\n",
    "            vm_usage_classes[key]['warm_start_cnt'] = warm_start_cnt_g_\n",
    "            vm_usage_classes[key]['idle_duration'] = idle_duration_g_\n",
    "            vm_usage_classes[key]['idle_vm_count'] = idle_vm_count_g_\n",
    "\n",
    "\n",
    "        \n",
    "    return vm_usage_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_stats(idle_vm_one_df, vm_usage_classes):\n",
    "    \n",
    "    '''\n",
    "    function to provide groups summary and stats\n",
    "    '''\n",
    "    \n",
    "    idle_vm_one_df_summary = idle_vm_one_df.groupby('group_number').agg({'idle_duration': sum,\n",
    "                                               'idle_vm_count': sum,\n",
    "                                              'vm_reused': sum}).reset_index()\n",
    "    \n",
    "    vm_usage_classes_df = pd.DataFrame(vm_usage_classes).transpose()\n",
    "    vm_usage_classes_df['group_number'] = vm_usage_classes_df.index\n",
    "\n",
    "    groups_summary = idle_vm_one_df_summary.merge(vm_usage_classes_df, on='group_number', how='inner')\n",
    "    \n",
    "    return groups_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groups_stats_v2(vm_usage_classes):\n",
    "    \n",
    "    '''\n",
    "    function to provide groups summary and stats\n",
    "    '''\n",
    "    \n",
    "    groups_summary = pd.DataFrame(vm_usage_classes).transpose()\n",
    "    groups_summary['group_number'] = groups_summary.index\n",
    "\n",
    "    \n",
    "    return groups_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Part g: add effects of min_idle_vm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_idle_vm_effect(min_idle_vm, total_idle_time, total_groups_distance, groups_summary):\n",
    "    if min_idle_vm> 0:\n",
    "\n",
    "        # effects of min_idle_vm on total idle time\n",
    "        total_idle_time += total_groups_distance*min_idle_vm \n",
    "        print(f'total_idle_time is {total_idle_time} sec')\n",
    "\n",
    "        # effects of min_idle_vm on vm start types counts and groups_summary df\n",
    "        groups_summary['vm_reused'] += min_idle_vm\n",
    "        groups_summary['cold_start_cnt'] -= min_idle_vm\n",
    "        groups_summary['warm_start_cnt'] += min_idle_vm\n",
    "        if min_idle_vm > 1:\n",
    "            groups_summary['cold_start_cnt_parallel'] -= min_idle_vm-1\n",
    "    # Note: min_idle_vm will only affects the first cluster of each group\n",
    "    \n",
    "    return total_idle_time, groups_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_idle_vm_effect_v2(min_idle_vm, total_idle_time_org, total_groups_distance, groups_summary_org):\n",
    "    \n",
    "    groups_summary = groups_summary_org.copy()\n",
    "    total_idle_time = total_idle_time_org\n",
    "    if min_idle_vm> 0:\n",
    "        # effects of min_idle_vm on total idle time\n",
    "        total_idle_time += total_groups_distance*min_idle_vm \n",
    "\n",
    "        # effects of min_idle_vm on vm start types counts and groups_summary df\n",
    "        groups_summary['cold_start_cnt'] -= min_idle_vm\n",
    "        groups_summary['warm_start_cnt'] += min_idle_vm\n",
    "        groups_summary['cold_start_cnt'][groups_summary.cold_start_cnt<0]=0\n",
    "    # Note: min_idle_vm will only affects the first cluster of each group\n",
    "    \n",
    "    return total_idle_time, groups_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improvements(greedy_df, base_pool_config):\n",
    "    \n",
    "    opt_config = {}\n",
    "    \n",
    "    opt_df = greedy_df[greedy_df.penalization == greedy_df.penalization.min()]\n",
    "    base_df = greedy_df[(greedy_df.auto_shutdown == base_pool_config['auto_shutdown']) & (greedy_df.min_idle_vm == base_pool_config['min_idle_vm'])]\n",
    "    \n",
    "    # percent improvements\n",
    "    percent_improvements = (base_df['penalization'].values[0] - opt_df['penalization'].values[0]) / base_df['penalization'].values[0]\n",
    "    \n",
    "    return np.round(100*percent_improvements,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def optimal_config(greedy_df, max_setup_time=300):\n",
    "    \n",
    "    # max_setup_time = 300 # five minutes\n",
    "    opt_config = {}\n",
    "    greedy_df['penalization']=(greedy_df.cold_start_cnt*max_setup_time + greedy_df.total_idle_time)/greedy_df.idle_vm_count\n",
    "    \n",
    "    opt_df = greedy_df[greedy_df.penalization == greedy_df.penalization.min()]\n",
    "    opt_config['auto_shutdown'] = opt_df['auto_shutdown'].values[0]\n",
    "    opt_config['min_idle_vm'] = opt_df['min_idle_vm'].values[0]\n",
    "    \n",
    "    return opt_config  # you have to add greedy_df here since you will use it in improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
